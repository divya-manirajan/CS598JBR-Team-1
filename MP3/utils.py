'''
Some utility functions that are used in the MP.

run_java:   Run a Java command and return the result.

get_prompt: Get the prompt for the model for a given entry, 
            vanilla setting and task.
'''

import subprocess
import re
import os
import glob

def get_java_code(response, java_dataset, entry, DELIMS, vanilla):
    """Extract the Java code from the response.

    Arguments:
    response -- A string generated by the LLM.
    java_dataset -- The Java HumanEval-X dataset.
    entry -- The entry we're evaluating in the python dataset.
    DELIMS -- A two item list, the start and end tags the model is supposed to wrap the java in.

    Returns:
    Java code formatted as a method in a Solution class for testing.
    """
    try:
        # java_raw = re.search(rf'{re.escape("[Java Start]")}(.*?){re.escape("[Java End]")}', response, re.DOTALL).group(1)
        # matches = re.findall(rf'{re.escape("[Java Start]")}(.*?){re.escape("[Java End]")}', response, re.DOTALL)
        matches = re.findall(rf'{re.escape(DELIMS[0])}(.*?){re.escape(DELIMS[1])}', response, re.DOTALL)
        if matches:
            java_raw = matches[-1]  # Get the last match
        else:
            print(f"No Java Code Found. DELIMS: {DELIMS}")
            return "No Java code found"
    except Exception as e:
        print(f"No Java Code Found. Exception: {e}")
        return "No Java code found"

    # Clean up java_raw to remove any unneeded pieces
    # not needed since we are using delimiters
    # if '```java' in java_raw:
    #     java_raw = java_raw.replace('```java', '')  # Remove the opening block
    # if '```' in java_raw:
    #     java_raw = java_raw.replace('```', '')  # Remove any remaining closing block

    # get the java canonical solution and test from humanevalx
    java_entry = [java_entry for java_entry in java_dataset if java_entry["task_id"] == entry["task_id"]][0]

    # Combine the imports from the response and canonical
    java_imports = re.findall(r"(import java.*?;)", java_raw)
    canonical_imports = re.findall(r"(import java.*?;)", java_entry['declaration'])
    combined_imports = '\n'.join(list(set(java_imports + canonical_imports)))

    # Assemble the new declaration with the combined imports and Solution class
    modified_declaration = combined_imports + '\n\nclass Solution {\n'

    # Extract the methods from the model response
            
    methods = []
    method_count = -1
    method_braces_count = 0
    in_method = False

    for j in java_raw.split('\n'):
        if ('public' in j or 'private' in j) and 'class' not in j:
            method_count += 1
            methods.append('')
            method_braces_count = 0
            in_method = True
        if in_method:
            methods[method_count] += j + '\n'
            method_braces_count += j.count('{') - j.count('}')
            if method_braces_count == 0:
                in_method = False
            
    # Filter out main method and any other classes 
    # We don't care if the model tries to provide a main method or solution class
    methods = [m for m in methods if ('Main' not in m) and ('main' not in m) and ('class' not in m)]

    # Assemble the final java code from the pieces
    java_extract = modified_declaration 
    java_extract += ''.join(methods) + '\n}\n' # Close the Solution class
    java_extract += java_entry["test"] 
                    
    return java_extract

def run_java(java_dir):
    current_dir = os.getcwd()
    os.chdir(java_dir)
    for file in glob.glob("*.class"):
        os.remove(file)
    command1 = ['javac', 'Main.java']
    command2 = ['java', 'Main']
    compile_result = subprocess.run(command1, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    run_result = subprocess.run(command2, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    os.chdir(current_dir)
    return [compile_result, run_result]

def java_output(task_id, compile_result, run_result):
    return f"""
For task {task_id}:
Compile rc     = {str(compile_result.returncode)} 
Compile stdout = {compile_result.stdout.decode('utf-8')} 
Compile stderr = {compile_result.stderr.decode('utf-8')} 

Run rc         = {str(run_result.returncode)}
Run stdout     = {run_result.stdout.decode('utf-8')}
Run stderr     = {run_result.stderr.decode('utf-8')}
        """

def get_prompt(entry, vanilla, task = 1):
    if vanilla: 
        prompt_file = "task_"+str(task)+"_vanilla_prompt.txt"
    else:
        prompt_file = "task_"+str(task)+"_crafted_prompt.txt"

    DELIMS = ['', '']
    with open(prompt_file, "r") as f:
        lines = f.readlines()
        prompt_raw = ''.join([l for l in lines if not l.startswith('%') and not l.startswith(':')])
        try:
            delim_s_setting = ''.join([l for l in lines if l.startswith(':DELIM_S')]).split(':')[2].strip()
            DELIMS[0] = delim_s_setting
        except:
            delim_s_setting = DELIMS[0]
        try:
            delim_e_setting = ''.join([l for l in lines if l.startswith(':DELIM_E:')]).split(':')[2].strip()
            DELIMS[1] = delim_e_setting
        except:
            delim_e_setting = DELIMS[1]
        prompt_delims = re.sub("DELIM_S", delim_s_setting, prompt_raw)
        prompt_delims = re.sub("DELIM_E", delim_e_setting, prompt_delims)
        # convert {variable_name} to {entry['variable_name']}
        prompt_w_entry = re.sub(r"\{(.*)\}", r"{entry['\1']}", prompt_delims)
        prompt = eval(f"f'''{prompt_w_entry}'''")
        
    return [prompt, DELIMS]


def accuracy(results):
    """Compute proportion of results that are correct."""
    total_count = len(results)
    correct_count = 0

    for result in results:
        if result["is_correct"]:
            correct_count += 1

    return correct_count / total_count
